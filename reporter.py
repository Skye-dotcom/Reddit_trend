"""æŠ¥å‘Šç”Ÿæˆæ¨¡å— - è´Ÿè´£è°ƒç”¨å¤§æ¨¡åž‹åˆ†æžå’Œç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""

import json
import logging
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List

from openai import OpenAI

from config import LLM_CONFIG, LLM_ANALYSIS_CONFIG


logger = logging.getLogger(__name__)


class ReportGenerator:
    """æŠ¥å‘Šç”Ÿæˆå™¨"""

    def __init__(self) -> None:
        self.llm_client = OpenAI(
            api_key=LLM_CONFIG.get("api_key"),
            base_url=LLM_CONFIG.get("base_url"),
        )
        logger.info("æŠ¥å‘Šç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆ")

    def generate_report(
        self,
        report_data: Dict[str, Any],
        output_dir: str = "reports",
    ) -> Dict[str, str]:
        """ç”Ÿæˆå®Œæ•´æŠ¥å‘Šå¹¶æŒ‰æ—¥æœŸç›®å½•å­˜å‚¨"""

        logger.info("å¼€å§‹ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š...")

        now = datetime.now()
        base_dir = Path(output_dir)
        date_dir = base_dir / f"{now:%Y}" / f"{now:%m}" / f"{now:%d}"
        date_dir.mkdir(parents=True, exist_ok=True)

        timestamp = now.strftime("%Y%m%d_%H%M%S")
        md_path = date_dir / f"report_{timestamp}.md"

        markdown_content = self._create_markdown_report(report_data)
        md_path.write_text(markdown_content, encoding="utf-8")

        latest_path = base_dir / "latest_report.md"
        latest_path.write_text(markdown_content, encoding="utf-8")

        logger.info("æŠ¥å‘Šå·²ç”Ÿæˆ: %s", md_path)

        return {
            "markdown": str(md_path),
            "latest": str(latest_path),
        }

    def analyze_with_llm(
        self,
        hot_ranking: List[Dict],
        trend_analysis: Dict[str, Any],
        detailed_posts: List[Dict],
    ) -> str:
        """ä½¿ç”¨å¤§æ¨¡åž‹è¿›è¡Œç»¼åˆåˆ†æžï¼ˆæ­¥éª¤8ä¸“ç”¨qwen3-maxï¼‰"""
        
        logger.info("å¼€å§‹å¤§æ¨¡åž‹ç»¼åˆåˆ†æžï¼ˆä½¿ç”¨%sï¼‰...", LLM_ANALYSIS_CONFIG.get("model"))
        
        # æ£€æŸ¥å¹¶è®°å½•Noneå€¼
        original_hot_count = len(hot_ranking)
        original_detailed_count = len(detailed_posts)
        
        none_in_hot = [i for i, post in enumerate(hot_ranking) if post is None]
        none_in_detailed = [i for i, post in enumerate(detailed_posts) if post is None]
        
        if none_in_hot or none_in_detailed:
            logger.error(f"\n{'='*60}")
            logger.error(f"âš ï¸ å‘çŽ°Noneå€¼ï¼")
            if none_in_hot:
                logger.error(f"hot_rankingä¸­æœ‰ {len(none_in_hot)} ä¸ªNoneå€¼ï¼Œä½ç½®: {none_in_hot}")
                logger.error(f"hot_rankingæ€»é•¿åº¦: {original_hot_count}")
            if none_in_detailed:
                logger.error(f"detailed_postsä¸­æœ‰ {len(none_in_detailed)} ä¸ªNoneå€¼ï¼Œä½ç½®: {none_in_detailed}")
                logger.error(f"detailed_postsæ€»é•¿åº¦: {original_detailed_count}")
            logger.error(f"{'='*60}\n")
        
        # è¿‡æ»¤Noneå€¼å¹¶è®°å½•è¢«è¿‡æ»¤çš„å¸–å­ä¿¡æ¯
        hot_ranking_filtered = []
        for i, post in enumerate(hot_ranking):
            if post is None:
                logger.error(f"âŒ hot_ranking[{i}] æ˜¯ Noneï¼Œå·²è¢«è¿‡æ»¤")
            else:
                hot_ranking_filtered.append(post)
        
        detailed_posts_filtered = []
        for i, post in enumerate(detailed_posts):
            if post is None:
                logger.error(f"âŒ detailed_posts[{i}] æ˜¯ Noneï¼Œå·²è¢«è¿‡æ»¤")
            else:
                detailed_posts_filtered.append(post)
        
        hot_ranking = hot_ranking_filtered
        detailed_posts = detailed_posts_filtered
        
        if not hot_ranking:
            logger.warning("çƒ­é—¨å¸–å­åˆ—è¡¨ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œåˆ†æž")
            return "æ•°æ®ä¸è¶³ï¼šæ²¡æœ‰å¯åˆ†æžçš„çƒ­é—¨å¸–å­"
        
        prompt = self._build_llm_prompt(hot_ranking, trend_analysis, detailed_posts)
        
        # å¦‚æžœLLM_ANALYSIS_CONFIGçš„api_keyä¸ºç©ºï¼Œä½¿ç”¨LLM_CONFIGçš„api_key
        api_key = LLM_ANALYSIS_CONFIG.get("api_key") or LLM_CONFIG.get("api_key")
        
        try:
            # ä¸ºæ­¥éª¤8åˆ›å»ºä¸“ç”¨çš„å®¢æˆ·ç«¯
            analysis_client = OpenAI(
                api_key=api_key,
                base_url=LLM_ANALYSIS_CONFIG.get("base_url"),
            )
            
            response = analysis_client.chat.completions.create(
                model=LLM_ANALYSIS_CONFIG.get("model"),
                messages=[
                    {
                        "role": "system",
                        "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIè¶‹åŠ¿åˆ†æžå¸ˆï¼Œæ“…é•¿ä»ŽRedditæ•°æ®ä¸­æ´žå¯ŸæŠ€æœ¯è¶‹åŠ¿ã€‚",
                    },
                    {"role": "user", "content": prompt},
                ],
                stream=True,
                temperature=LLM_ANALYSIS_CONFIG.get("temperature", 0.3),
                max_tokens=LLM_ANALYSIS_CONFIG.get("max_tokens", 10000),
            )
            
            content = ""
            for chunk in response:
                delta = chunk.choices[0].delta.content
                if delta:
                    content += delta
            
            logger.info("å¤§æ¨¡åž‹åˆ†æžå®Œæˆ")
            return content
        
        except Exception as exc:  # pylint: disable=broad-except
            logger.error("å¤§æ¨¡åž‹åˆ†æžå¤±è´¥: %s", exc)
            return f"åˆ†æžå¤±è´¥: {exc}"
    
    def _build_llm_prompt(self, hot_ranking: List[Dict],
                         trend_analysis: Dict[str, Any],
                         detailed_posts: List[Dict]) -> str:
        """æž„å»ºå¤§æ¨¡åž‹åˆ†æžæç¤º"""
        
        # äºŒæ¬¡æ£€æŸ¥Noneå€¼ï¼ˆç†è®ºä¸Šä¸åº”è¯¥æœ‰ï¼Œå› ä¸ºanalyze_with_llmå·²ç»è¿‡æ»¤äº†ï¼‰
        none_count_hot = sum(1 for post in hot_ranking if post is None)
        none_count_detailed = sum(1 for post in detailed_posts if post is None)
        
        if none_count_hot > 0 or none_count_detailed > 0:
            logger.error(f"âš ï¸ _build_llm_promptä¸­ä»ç„¶å‘çŽ°Noneå€¼ï¼")
            logger.error(f"   hot_ranking: {none_count_hot}ä¸ªNone / {len(hot_ranking)}ä¸ªæ€»æ•°")
            logger.error(f"   detailed_posts: {none_count_detailed}ä¸ªNone / {len(detailed_posts)}ä¸ªæ€»æ•°")
            
            # è®°å½•Noneå€¼çš„å…·ä½“ä½ç½®
            for i, post in enumerate(hot_ranking):
                if post is None:
                    logger.error(f"   hot_ranking[{i}] = None")
            for i, post in enumerate(detailed_posts):
                if post is None:
                    logger.error(f"   detailed_posts[{i}] = None")
        
        # è¿‡æ»¤æŽ‰Noneå€¼ï¼ˆé˜²å¾¡æ€§ç¼–ç¨‹ï¼‰
        hot_ranking = [post for post in hot_ranking if post is not None]
        detailed_posts = [post for post in detailed_posts if post is not None]
        
        # æå–çƒ­é—¨å¸–å­æ‘˜è¦
        hot_summary = []
        for i, post in enumerate(hot_ranking[:10], 1):
            # ä¼˜å…ˆä½¿ç”¨æ‘˜è¦ï¼Œå¦‚æžœæ²¡æœ‰æˆ–ç”Ÿæˆå¤±è´¥åˆ™ä½¿ç”¨æ ‡é¢˜
            if post.get('summary'):
                summary_text = post['summary'][:80]
            else:
                summary_text = post.get('title', '')[:80]
            hot_summary.append(f"{i}. [{summary_text}] - r/{post['subreddit']} - {post['score']}åˆ† - {post['num_comments']}è¯„è®º")
        
        # æå–TOP5è¯¦ç»†å†…å®¹
        detailed_summary = []
        for i, post in enumerate(detailed_posts, 1):
            content_preview = post.get('content', '')[:500] if post.get('content') else "æ— å†…å®¹"
            detailed_summary.append(f"""
### é«˜è´¨é‡å¸–å­ {i}: {post['title']}
- ç¤¾åŒº: r/{post['subreddit']}
- åˆ†æ•°: {post['score']} | è¯„è®º: {post['num_comments']}
- è´¨é‡è¯„åˆ†: {post.get('quality_score', 'N/A')}
- å†…å®¹æ‘˜è¦: {content_preview}
- è¯„è®ºæ•°: {len(post.get('comments', []))}
""")
        
        prompt = f"""
å½“å‰æ—¥æœŸ: {datetime.now().strftime("%Y-%m-%d")}

# Reddit AIç¤¾åŒºè¶‹åŠ¿åˆ†æžä»»åŠ¡

ä½ éœ€è¦åŸºäºŽä»¥ä¸‹Redditç¤¾åŒºæ•°æ®ï¼Œè¿›è¡Œæ·±å…¥çš„è¶‹åŠ¿åˆ†æžå¹¶ç”Ÿæˆä¸“ä¸šæŠ¥å‘Šã€‚

## æ•°æ®æ¦‚è§ˆ

### çƒ­é—¨å¸–å­TOP10
{chr(10).join(hot_summary)}

### è¶‹åŠ¿å…³é”®è¯
{json.dumps(trend_analysis.get('keyword_trends', {}), ensure_ascii=False, indent=2)}

### ç¤¾åŒºè¡¨çŽ°
{json.dumps(trend_analysis.get('subreddit_trends', {}), ensure_ascii=False, indent=2)}

### æ´»è·ƒä½œè€…
{json.dumps(trend_analysis.get('author_trends', {}), ensure_ascii=False, indent=2)}

### äº’åŠ¨è¶‹åŠ¿
{json.dumps(trend_analysis.get('engagement_trends', {}), ensure_ascii=False, indent=2)}

## é«˜è´¨é‡å¸–å­è¯¦ç»†å†…å®¹

{chr(10).join(detailed_summary)}

---

## åˆ†æžè¦æ±‚

è¯·åŸºäºŽä»¥ä¸Šæ•°æ®ï¼Œæ’°å†™ä¸€ä»½ä¸“ä¸šçš„è¶‹åŠ¿åˆ†æžæŠ¥å‘Šï¼ŒåŒ…å«ä»¥ä¸‹éƒ¨åˆ†ï¼š

### 1. æ ¸å¿ƒçƒ­ç‚¹è¯é¢˜è¯†åˆ«ï¼ˆ3-5ä¸ªï¼‰
- æ¯ä¸ªçƒ­ç‚¹çš„è¯¦ç»†æè¿°
- ç›¸å…³å¸–å­ç»Ÿè®¡å’Œè¶‹åŠ¿
- ç¤¾åŒºè®¨è®ºçƒ­åº¦
- æŠ€æœ¯æˆ–åº”ç”¨å±‚é¢çš„é‡è¦æ€§

### 2. æ–°å…´è¶‹åŠ¿å‘çŽ°ï¼ˆ2-3ä¸ªï¼‰
- æ–°å‡ºçŽ°çš„æŠ€æœ¯è¶‹åŠ¿æˆ–è®¨è®ºä¸»é¢˜
- å¢žé•¿æ½œåŠ›åˆ†æž
- å¯èƒ½çš„æœªæ¥å‘å±•

### 3. æŠ€æœ¯æ·±åº¦æ´žå¯Ÿ
- åŸºäºŽé«˜è´¨é‡å¸–å­çš„æ·±åº¦åˆ†æž
- æŠ€æœ¯å‘å±•çš„ç“¶é¢ˆå’Œæœºé‡
- å¯¹æœªæ¥å‘å±•çš„é¢„æµ‹

### 4. ç¤¾åŒºç”Ÿæ€è§‚å¯Ÿ
- ä¸åŒç¤¾åŒºçš„ç‰¹ç‚¹å’Œä¸“é•¿
- è·¨ç¤¾åŒºçš„å…±åŒå…³æ³¨ç‚¹
- ç¤¾åŒºé—´çš„å·®å¼‚å’Œç‰¹è‰²

### 5. è¡ŒåŠ¨å»ºè®®
- å¯¹å¼€å‘è€…/ç ”ç©¶è€…çš„å»ºè®®
- å€¼å¾—å…³æ³¨çš„æ–¹å‘
- æ½œåœ¨çš„æœºä¼šç‚¹

## è¾“å‡ºæ ¼å¼

è¯·ä½¿ç”¨æ¸…æ™°çš„Markdownæ ¼å¼ï¼Œç”¨å…·ä½“çš„æ•°æ®å’Œå®žä¾‹æ”¯æŒä½ çš„åˆ†æžã€‚ç¡®ä¿åˆ†æžæ·±å…¥ã€æ•°æ®é©±åŠ¨ï¼Œå¹¶æä¾›å¯æ“ä½œçš„æ´žå¯Ÿã€‚
"""
        
        return prompt
    
    def _create_markdown_report(self, report_data: Dict[str, Any]) -> str:
        """åˆ›å»ºMarkdownæŠ¥å‘Š"""
    
        metadata = report_data.get('metadata', {})
        timeframe_rankings = report_data.get('timeframe_rankings', {})
        quality_ranking = report_data.get('quality_ranking', [])
        trend_analysis = report_data.get('trend_analysis', {})
        llm_analysis = report_data.get('llm_analysis', '')
        
        # è¿‡æ»¤Noneå€¼
        timeframe_rankings = {
            key: [post for post in posts if post is not None]
            for key, posts in timeframe_rankings.items()
        }
        quality_ranking = [post for post in quality_ranking if post is not None]
    
        report = f"""# Reddit AIç¤¾åŒºæ·±åº¦åˆ†æžæŠ¥å‘Š

    **ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}  
    **æ•°æ®æ”¶é›†æ—¶é—´**: {metadata.get('start_time', 'N/A')}  
    **åˆ†æžè€—æ—¶**: {metadata.get('duration', 0):.1f}ç§’

---

## ðŸ“Š æ•°æ®æ¦‚è§ˆ

- **å½“å¤©çƒ­é—¨å¸–å­**: {len(timeframe_rankings.get('hot', []))} æ¡
- **æœ¬å‘¨çƒ­é—¨å¸–å­**: {len(timeframe_rankings.get('week', []))} æ¡  
- **æœ¬æœˆçƒ­é—¨å¸–å­**: {len(timeframe_rankings.get('month', []))} æ¡
- **é«˜è´¨é‡æ·±åº¦åˆ†æž**: {len(quality_ranking)} æ¡
- **è¦†ç›–ç¤¾åŒº**: {trend_analysis.get('subreddit_trends', {}).get('total_subreddits', 0)} ä¸ª
- **æ´»è·ƒä½œè€…**: {trend_analysis.get('author_trends', {}).get('active_authors', 0)} ä½

---

## ðŸ”¥ å½“å¤©çƒ­é—¨å¸–å­æŽ’è¡Œæ¦œ (å®žæ—¶çƒ­åº¦)

| æŽ’å | æ ‡é¢˜ | ç¤¾åŒº | åˆ†æ•° | è¯„è®ºæ•° |
|------|------|------|------|--------|
"""
    
        # æ·»åŠ å½“å¤©çƒ­é—¨æŽ’è¡Œ
        for i, post in enumerate(timeframe_rankings.get('hot', [])[:20], 1):
            post_url = post.get('permalink', f"https://reddit.com/comments/{post['id']}")
            title = self._escape_markdown(post['title'][:60])
            if len(post['title']) > 60:
                title += "..."
        
            report += f"| {i} | [{title}]({post_url}) | "
            report += f"r/{post['subreddit']} | {post['score']} | "
            report += f"{post['num_comments']} |\n"
            # æ·»åŠ æ‘˜è¦æˆ–é”™è¯¯ä¿¡æ¯
            if post.get('summary_error'):
                error_msg = f"âš ï¸ æ‘˜è¦ç”Ÿæˆé”™è¯¯ï¼š{post.get('summary_error')}"
                error_escaped = self._escape_markdown(error_msg)
                report += f"| | {error_escaped} | | | |\n"
            elif post.get('summary'):
                summary_escaped = self._escape_markdown(post['summary'])
                report += f"| | {summary_escaped} | | | |\n"
    
        report += "\n---\n\n## ðŸ“ˆ æœ¬å‘¨çƒ­é—¨å¸–å­æŽ’è¡Œæ¦œ (æŒ‰åˆ†æ•°æŽ’åº)\n\n"
        report += "| æŽ’å | æ ‡é¢˜ | ç¤¾åŒº | åˆ†æ•° | è¯„è®ºæ•° |\n"
        report += "|------|------|------|------|--------|\n"
    
        for i, post in enumerate(timeframe_rankings.get('week', [])[:20], 1):
            post_url = post.get('permalink', f"https://reddit.com/comments/{post['id']}")
            title = self._escape_markdown(post['title'][:60])
            if len(post['title']) > 60:
                title += "..."
        
            report += f"| {i} | [{title}]({post_url}) | "
            report += f"r/{post['subreddit']} | {post['score']} | "
            report += f"{post['num_comments']} |\n"
            # æ·»åŠ æ‘˜è¦æˆ–é”™è¯¯ä¿¡æ¯
            if post.get('summary_error'):
                error_msg = f"âš ï¸ æ‘˜è¦ç”Ÿæˆé”™è¯¯ï¼š{post.get('summary_error')}"
                error_escaped = self._escape_markdown(error_msg)
                report += f"| | {error_escaped} | | | |\n"
            elif post.get('summary'):
                summary_escaped = self._escape_markdown(post['summary'])
                report += f"| | {summary_escaped} | | | |\n"
    
        report += "\n---\n\n## ðŸ—“ï¸ æœ¬æœˆçƒ­é—¨å¸–å­æŽ’è¡Œæ¦œ (æŒ‰åˆ†æ•°æŽ’åº)\n\n"
        report += "| æŽ’å | æ ‡é¢˜ | ç¤¾åŒº | åˆ†æ•° | è¯„è®ºæ•° |\n"
        report += "|------|------|------|------|--------|\n"
    
        for i, post in enumerate(timeframe_rankings.get('month', [])[:20], 1):
            post_url = post.get('permalink', f"https://reddit.com/comments/{post['id']}")
            title = self._escape_markdown(post['title'][:60])
            if len(post['title']) > 60:
                title += "..."
        
            report += f"| {i} | [{title}]({post_url}) | "
            report += f"r/{post['subreddit']} | {post['score']} | "
            report += f"{post['num_comments']} |\n"
            # æ·»åŠ æ‘˜è¦æˆ–é”™è¯¯ä¿¡æ¯
            if post.get('summary_error'):
                error_msg = f"âš ï¸ æ‘˜è¦ç”Ÿæˆé”™è¯¯ï¼š{post.get('summary_error')}"
                error_escaped = self._escape_markdown(error_msg)
                report += f"| | {error_escaped} | | | |\n"
            elif post.get('summary'):
                summary_escaped = self._escape_markdown(post['summary'])
                report += f"| | {summary_escaped} | | | |\n"

        # ... å…¶ä½™æŠ¥å‘Šå†…å®¹ä¿æŒä¸å˜
        
        # æ·»åŠ é«˜è´¨é‡æŽ’è¡Œ
        report += "\n---\n\n## â­ é«˜è´¨é‡å¸–å­æ·±åº¦åˆ†æž\n\n"
        report += "| æŽ’å | æ ‡é¢˜ | ç¤¾åŒº | è´¨é‡è¯„åˆ† | åˆ†æ•° | è¯„è®ºæ•° |\n"
        report += "|------|------|------|----------|------|--------|\n"
        
        for i, post in enumerate(quality_ranking, 1):
            post_url = post.get('permalink', f"https://reddit.com/comments/{post['id']}")
            title = self._escape_markdown(post['title'][:50])
            if len(post['title']) > 50:
                title += "..."
            
            report += f"| {i} | [{title}]({post_url}) | "
            report += f"r/{post['subreddit']} | {post.get('quality_score', 0):.2f} | "
            report += f"{post['score']} | {post['num_comments']} |\n"
            # æ·»åŠ æ‘˜è¦æˆ–é”™è¯¯ä¿¡æ¯
            if post.get('summary_error'):
                error_msg = f"âš ï¸ æ‘˜è¦ç”Ÿæˆé”™è¯¯ï¼š{post.get('summary_error')}"
                error_escaped = self._escape_markdown(error_msg)
                report += f"| | {error_escaped} | | | | |\n"
            elif post.get('summary'):
                summary_escaped = self._escape_markdown(post['summary'])
                report += f"| | {summary_escaped} | | | | |\n"
        
        # æ·»åŠ è¶‹åŠ¿å…³é”®è¯
        keyword_freq = trend_analysis.get('keyword_trends', {}).get('keyword_frequency', {})
        trending_kw = trend_analysis.get('keyword_trends', {}).get('trending_keywords', [])
        
        report += "\n---\n\n## ðŸ” è¶‹åŠ¿å…³é”®è¯\n\n"
        report += "| å…³é”®è¯ | å‡ºçŽ°é¢‘çŽ‡ | è¶‹åŠ¿çº§åˆ« |\n"
        report += "|--------|----------|----------|\n"
        
        for keyword, freq in list(keyword_freq.items())[:15]:
            trend_label = "ðŸ”¥ çƒ­é—¨" if keyword in trending_kw[:5] else "ðŸ“ˆ ä¸Šå‡" if keyword in trending_kw else "âž¡ï¸ ä¸€èˆ¬"
            report += f"| {keyword} | {freq} | {trend_label} |\n"
        
        # æ·»åŠ å¤§æ¨¡åž‹åˆ†æž
        report += "\n---\n\n# ðŸ¤– AIæ™ºèƒ½æ·±åº¦åˆ†æž\n\n"
        report += llm_analysis
        
        # æ·»åŠ é™„å½•
        report += "\n\n---\n\n## ðŸ“Œ é™„å½•\n\n"
        report += "### ç¤¾åŒºè¡¨çŽ°ç»Ÿè®¡\n\n"
        
        subreddit_perf = trend_analysis.get('subreddit_trends', {}).get('subreddit_performance', {})
        for sub, stats in list(subreddit_perf.items())[:10]:
            report += f"- **r/{sub}**: {stats['posts']}ä¸ªå¸–å­, å¹³å‡åˆ†æ•° {stats['avg_score']:.1f}\n"
        
        report += f"\n\n---\n\n*æŠ¥å‘Šç”±Redditæ™ºèƒ½åˆ†æžç³»ç»Ÿç”Ÿæˆ*  \n*æ•°æ®æ¥æº: Reddit API*"
        
        return report
    
    def _escape_markdown(self, text: str) -> str:
        """è½¬ä¹‰Markdownç‰¹æ®Šå­—ç¬¦"""
        if not text:
            return ""
        
        escape_chars = ['|', '[', ']', '(', ')', '*', '_', '`']
        for char in escape_chars:
            text = text.replace(char, f'\\{char}')
        
        text = text.replace('\n', ' ').replace('\r', ' ')
        
        return text
