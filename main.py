"""
Redditç¤¾åŒºæ•°æ®åˆ†æç³»ç»Ÿ - ä¸»ç¨‹åº
æµç¨‹ï¼šè·å– -> æ¸…æ´— -> åˆ†æ -> è¯„åˆ† -> æ·±åº¦æŠ“å– -> ç»¼åˆæŠ¥å‘Š
"""

import logging
from datetime import datetime
from fetcher import RedditDataFetcher
from cleaner import DataCleaner
from analyzer import TrendAnalyzer
from scorer import QualityScorer
from reporter import ReportGenerator
from summarizer import PostSummarizer

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ç¤¾åŒºä¼˜å…ˆçº§é…ç½®
SUBREDDIT_CONFIG = {
    "high": [
        {"name": "LocalLLaMA", "limit": 50},
        {"name": "MachineLearning", "limit": 50},
        {"name": "singularity", "limit": 40},
        {"name": "OpenAI", "limit": 40},
    ],
    "medium": [
        {"name": "LangChain", "limit": 30},
        {"name": "artificial", "limit": 25},
    ]
}

def main():
    """ä¸»æµç¨‹"""
    print("=" * 60)
    print("Reddit AIç¤¾åŒºæ·±åº¦åˆ†æç³»ç»Ÿ")
    print("=" * 60)
    
    start_time = datetime.now()
    
    # åˆå§‹åŒ–å„æ¨¡å—
    fetcher = RedditDataFetcher()
    cleaner = DataCleaner()
    summarizer = PostSummarizer()  # åˆå§‹åŒ–æ‘˜è¦ç”Ÿæˆå™¨
    analyzer = TrendAnalyzer(summarizer=summarizer)  # ä¼ å…¥summarizer
    scorer = QualityScorer()
    reporter = ReportGenerator()
    
    # ========== æ­¥éª¤1: è·å–åŸºç¡€å¸–å­ä¿¡æ¯ ==========
    logger.info("æ­¥éª¤1: è·å–åŸºç¡€å¸–å­ä¿¡æ¯")
    raw_posts = fetcher.fetch_posts_from_subreddits(SUBREDDIT_CONFIG)
    logger.info(f"å…±è·å– {sum(len(posts) for posts in raw_posts.values())} ä¸ªå¸–å­")
    
    # ========== æ­¥éª¤2: æ•°æ®æ¸…æ´—ï¼ˆä¸å»é‡ï¼‰==========
    logger.info("æ­¥éª¤2: æ•°æ®æ¸…æ´—")
    cleaned_posts = cleaner.clean_posts(raw_posts, remove_duplicates=False)
    logger.info(f"æ¸…æ´—åä¿ç•™ {sum(len(posts) for posts in cleaned_posts.values())} ä¸ªå¸–å­")
    
    # ========== æ­¥éª¤3: åˆ¶ä½œä¸‰ä¸ªæ—¶é—´ç»´åº¦çš„çƒ­é—¨æ’è¡Œè¡¨ + è¶‹åŠ¿åˆ†æ + ç”Ÿæˆæ‘˜è¦ ==========
    logger.info("æ­¥éª¤3: ä¸‰ä¸ªæ—¶é—´ç»´åº¦çƒ­é—¨æ’è¡Œ + è¶‹åŠ¿åˆ†æ + æ‘˜è¦ç”Ÿæˆ")
    timeframe_rankings = analyzer.create_hot_ranking(
        cleaned_posts, 
        top_k=20, 
        fetcher=fetcher,  # ä¼ å…¥fetcherç”¨äºè·å–è¯„è®º
        generate_summaries=True  # å¯ç”¨æ‘˜è¦ç”Ÿæˆ
    )
    trend_analysis = analyzer.analyze_trends(cleaned_posts)
    logger.info(f"ç”Ÿæˆçƒ­é—¨æ’è¡Œæ¦œ: hot-{len(timeframe_rankings['hot'])}, week-{len(timeframe_rankings['week'])}, month-{len(timeframe_rankings['month'])}")
    logger.info("æ‘˜è¦ç”Ÿæˆå·²å®Œæˆ")
    
    # ========== æ­¥éª¤4: å»é‡ï¼ˆä¿ç•™hotæœ€é«˜ï¼‰==========
    logger.info("æ­¥éª¤4: æ•°æ®å»é‡")
    unique_posts = cleaner.deduplicate_posts(cleaned_posts, keep='highest_hot')
    logger.info(f"å»é‡åä¿ç•™ {len(unique_posts)} ä¸ªå”¯ä¸€å¸–å­")
    
    # ========== æ­¥éª¤5: è´¨é‡è¯„åˆ†æ’åº ==========
    logger.info("æ­¥éª¤5: è´¨é‡è¯„åˆ†")
    scored_posts = scorer.score_posts(unique_posts, trend_analysis)
    quality_ranking = scorer.get_top_quality_posts(scored_posts, top_k=5)
    logger.info(f"é«˜è´¨é‡å¸–å­TOP5å·²é€‰å‡º")
    
    # ä¸ºé«˜è´¨é‡å¸–å­ç”Ÿæˆæ‘˜è¦ï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰
    logger.info("ä¸ºé«˜è´¨é‡å¸–å­ç”Ÿæˆæ‘˜è¦...")
    quality_ranking = summarizer.generate_summaries_for_posts(
        quality_ranking, 
        fetcher,
        max_workers=3,  # TOP5æ•°é‡å°‘ï¼Œå‡å°‘å¹¶å‘
        max_comments=5
    )
    logger.info("é«˜è´¨é‡å¸–å­æ‘˜è¦ç”Ÿæˆå®Œæˆ")
    
    # ========== æ­¥éª¤6: æ·±åº¦ä¿¡æ¯è·å–ï¼ˆTOP5ï¼‰==========
    logger.info("æ­¥éª¤6: æ·±åº¦ä¿¡æ¯è·å–")
    top5_ids = [post['id'] for post in quality_ranking]
    detailed_posts = fetcher.fetch_detailed_posts(top5_ids, comment_depth=2)
    logger.info(f"æˆåŠŸè·å– {len(detailed_posts)} ä¸ªå¸–å­çš„è¯¦ç»†ä¿¡æ¯")
    
    # ========== æ­¥éª¤7: å¤§æ¨¡å‹ç»¼åˆåˆ†æ ==========
    logger.info("æ­¥éª¤7: å¤§æ¨¡å‹ç»¼åˆåˆ†æ")
    
    # æ•°æ®å®Œæ•´æ€§éªŒè¯
    logger.info(f"\n{'='*60}")
    logger.info("ğŸ“Š æ•°æ®å®Œæ•´æ€§éªŒè¯...")
    logger.info(f"  hotæ’è¡Œæ¦œ: {len(timeframe_rankings['hot'])} ä¸ªå¸–å­")
    logger.info(f"  weekæ’è¡Œæ¦œ: {len(timeframe_rankings['week'])} ä¸ªå¸–å­")
    logger.info(f"  monthæ’è¡Œæ¦œ: {len(timeframe_rankings['month'])} ä¸ªå¸–å­")
    logger.info(f"  è¯¦ç»†å¸–å­: {len(detailed_posts)} ä¸ªå¸–å­")
    
    # æ£€æŸ¥Noneå€¼
    none_in_hot = []
    for i, post in enumerate(timeframe_rankings['hot']):
        if post is None:
            none_in_hot.append(i)
            logger.error(f"âŒ hotæ’è¡Œæ¦œ[{i}] æ˜¯ None")
    
    none_in_week = []
    for i, post in enumerate(timeframe_rankings['week']):
        if post is None:
            none_in_week.append(i)
            logger.error(f"âŒ weekæ’è¡Œæ¦œ[{i}] æ˜¯ None")
    
    none_in_month = []
    for i, post in enumerate(timeframe_rankings['month']):
        if post is None:
            none_in_month.append(i)
            logger.error(f"âŒ monthæ’è¡Œæ¦œ[{i}] æ˜¯ None")
    
    none_in_detailed = []
    for i, post in enumerate(detailed_posts):
        if post is None:
            none_in_detailed.append(i)
            logger.error(f"âŒ detailed_posts[{i}] æ˜¯ None")
    
    total_none = len(none_in_hot) + len(none_in_week) + len(none_in_month) + len(none_in_detailed)
    
    if total_none > 0:
        logger.error(f"\nâš ï¸ å‘ç° {total_none} ä¸ªNoneå€¼ï¼")
        if none_in_hot:
            logger.error(f"  hotæ’è¡Œæ¦œä¸­æœ‰ {len(none_in_hot)} ä¸ªNoneï¼Œä½ç½®: {none_in_hot}")
        if none_in_week:
            logger.error(f"  weekæ’è¡Œæ¦œä¸­æœ‰ {len(none_in_week)} ä¸ªNoneï¼Œä½ç½®: {none_in_week}")
        if none_in_month:
            logger.error(f"  monthæ’è¡Œæ¦œä¸­æœ‰ {len(none_in_month)} ä¸ªNoneï¼Œä½ç½®: {none_in_month}")
        if none_in_detailed:
            logger.error(f"  detailed_postsä¸­æœ‰ {len(none_in_detailed)} ä¸ªNoneï¼Œä½ç½®: {none_in_detailed}")
    else:
        logger.info("âœ… æ•°æ®éªŒè¯é€šè¿‡ï¼Œæ²¡æœ‰Noneå€¼")
    
    logger.info(f"{'='*60}\n")

    # åˆ›å»ºä¸€ä¸ªåˆå¹¶çš„æ’è¡Œæ¦œï¼ŒåŒ…å«æ‰€æœ‰ä¸‰ä¸ªæ—¶é—´ç»´åº¦çš„å¸–å­
    combined_ranking = (
        timeframe_rankings['hot'] + 
        timeframe_rankings['week'] + 
        timeframe_rankings['month']
    )
    
    logger.info(f"åˆå¹¶åæ’è¡Œæ¦œæ€»æ•°: {len(combined_ranking)} ä¸ªå¸–å­")

    llm_analysis = reporter.analyze_with_llm(
        hot_ranking=combined_ranking,  # ä¼ é€’åˆå¹¶åçš„æ’è¡Œæ¦œ
        trend_analysis=trend_analysis,
        detailed_posts=detailed_posts
    )
    
    # ========== æ­¥éª¤8: ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š ==========
    logger.info("æ­¥éª¤8: ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š")
    report_data = {
        'timeframe_rankings': timeframe_rankings,  # è¿™é‡Œæ”¹ä¸ºtimeframe_rankings
        'quality_ranking': quality_ranking,
        'trend_analysis': trend_analysis,
        'detailed_posts': detailed_posts,
        'llm_analysis': llm_analysis,
        'metadata': {
            'start_time': start_time.isoformat(),
            'end_time': datetime.now().isoformat(),
            'duration': (datetime.now() - start_time).total_seconds()
        }
    }   
    
    report_files = reporter.generate_report(report_data)
    
    # ========== å®Œæˆ ==========
    duration = (datetime.now() - start_time).total_seconds()
    print("\n" + "=" * 60)
    print("åˆ†æå®Œæˆ!")
    print(f"è€—æ—¶: {duration:.1f}ç§’")
    print(f"MarkdownæŠ¥å‘Š: {report_files.get('markdown', 'N/A')}")
    print("=" * 60)

if __name__ == "__main__":
    main()
